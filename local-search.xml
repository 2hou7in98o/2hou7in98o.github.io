<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>pytorch&lt;1&gt;:数据操作</title>
    <link href="/2022/03/20/pytorch1/"/>
    <url>/2022/03/20/pytorch1/</url>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><div class="code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br></code></pre></td></tr></table></figure><h1 id="1-标量"><a href="#1-标量" class="headerlink" title="1.标量"></a>1.标量</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.tensor(<span class="hljs-number">2.0</span>)<br>y = torch.tensor(<span class="hljs-number">3.5</span>)<br>x + y, x - y, x * y , x/ y , x**y , x.shape <span class="hljs-comment"># 注意：标量无轴</span><br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">(tensor(5.5000), tensor(-1.5000), tensor(7.), tensor(0.5714), tensor(11.3137), torch.Size([]))</code></pre></div><h1 id="2-向量"><a href="#2-向量" class="headerlink" title="2.向量"></a>2.向量</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.arange(<span class="hljs-number">4</span>)<br>x<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">tensor([0, 1, 2, 3])</code></pre></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x[<span class="hljs-number">3</span>]<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">tensor(3)</code></pre></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">len</span>(x), x.shape<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">(4, torch.Size([4]))</code></pre></div><h1 id="3-矩阵"><a href="#3-矩阵" class="headerlink" title="3.矩阵"></a>3.矩阵</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">A = torch.arange(<span class="hljs-number">20</span>).reshape(<span class="hljs-number">5</span>,<span class="hljs-number">4</span>)<br>A<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">tensor([[ 0,  1,  2,  3],        [ 4,  5,  6,  7],        [ 8,  9, 10, 11],        [12, 13, 14, 15],        [16, 17, 18, 19]])</code></pre></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">A.T  <span class="hljs-comment"># 矩阵转置</span><br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">tensor([[ 0,  4,  8, 12, 16],        [ 1,  5,  9, 13, 17],        [ 2,  6, 10, 14, 18],        [ 3,  7, 11, 15, 19]])</code></pre></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">B = torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>] , [<span class="hljs-number">2</span>,<span class="hljs-number">0</span>,<span class="hljs-number">4</span>] , [<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>]])<br>B<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">tensor([[1, 2, 3],        [2, 0, 4],        [3, 4, 5]])</code></pre></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">B == B.T   <span class="hljs-comment"># 对称矩阵</span><br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">tensor([[True, True, True],        [True, True, True],        [True, True, True]])</code></pre></div><h1 id="4-张量"><a href="#4-张量" class="headerlink" title="4.张量"></a>4.张量</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.arange(<span class="hljs-number">24</span>).reshape(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br>x<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">tensor([[[ 0,  1,  2,  3],         [ 4,  5,  6,  7],         [ 8,  9, 10, 11]],        [[12, 13, 14, 15],         [16, 17, 18, 19],         [20, 21, 22, 23]]])</code></pre></div><h1 id="5-张量算法的基本性质"><a href="#5-张量算法的基本性质" class="headerlink" title="5.张量算法的基本性质"></a>5.张量算法的基本性质</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">A = torch.arange(<span class="hljs-number">20</span>,dtype = torch.float32).reshape(<span class="hljs-number">5</span>,<span class="hljs-number">4</span>)<br>B = A.clone()<br>A , A+B<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">(tensor([[ 0.,  1.,  2.,  3.],         [ 4.,  5.,  6.,  7.],         [ 8.,  9., 10., 11.],         [12., 13., 14., 15.],         [16., 17., 18., 19.]]), tensor([[ 0.,  2.,  4.,  6.],         [ 8., 10., 12., 14.],         [16., 18., 20., 22.],         [24., 26., 28., 30.],         [32., 34., 36., 38.]]))</code></pre></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#  hadamard product</span><br>A * B<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">tensor([[  0.,   1.,   4.,   9.],        [ 16.,  25.,  36.,  49.],        [ 64.,  81., 100., 121.],        [144., 169., 196., 225.],        [256., 289., 324., 361.]])</code></pre></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 张量与标量运算</span><br>a = <span class="hljs-number">2</span><br>X = torch.arange(<span class="hljs-number">24</span>,dtype=torch.float32).reshape(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br>a + X , (a * X).shape<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">(tensor([[[ 2.,  3.,  4.,  5.],          [ 6.,  7.,  8.,  9.],          [10., 11., 12., 13.]],          [[14., 15., 16., 17.],          [18., 19., 20., 21.],          [22., 23., 24., 25.]]]), torch.Size([2, 3, 4]))</code></pre></div><h1 id="6-降维"><a href="#6-降维" class="headerlink" title="6.降维"></a>6.降维</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">x = torch.arange(<span class="hljs-number">24</span>,dtype=torch.float32).reshape(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br>x, x.<span class="hljs-built_in">sum</span>()<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">(tensor([[[ 0.,  1.,  2.,  3.],          [ 4.,  5.,  6.,  7.],          [ 8.,  9., 10., 11.]],          [[12., 13., 14., 15.],          [16., 17., 18., 19.],          [20., 21., 22., 23.]]]), tensor(276.))</code></pre></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 按照轴求和</span><br>x.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">0</span>),x.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">0</span>).shape, x.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>) , x.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>).shape<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">(tensor([[12., 14., 16., 18.],         [20., 22., 24., 26.],         [28., 30., 32., 34.]]), torch.Size([3, 4]), tensor([[12., 15., 18., 21.],         [48., 51., 54., 57.]]), torch.Size([2, 4]))</code></pre></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x.<span class="hljs-built_in">sum</span>(axis = [<span class="hljs-number">0</span>,<span class="hljs-number">1</span>] )<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">tensor([60., 66., 72., 78.])</code></pre></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 求平均值</span><br>x.mean() , x.<span class="hljs-built_in">sum</span>() / x.numel()<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">(tensor(11.5000), tensor(11.5000))</code></pre></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 同样的也可以按照轴求</span><br>x.mean(axis=<span class="hljs-number">0</span>) , x.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">0</span>) / x.shape[<span class="hljs-number">0</span>] <br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">(tensor([[ 6.,  7.,  8.,  9.],         [10., 11., 12., 13.],         [14., 15., 16., 17.]]), tensor([[ 6.,  7.,  8.,  9.],         [10., 11., 12., 13.],         [14., 15., 16., 17.]]))</code></pre></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 非降维求和</span><br>x.shape, x.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">0</span>).shape , x.<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">0</span>,keepdims=<span class="hljs-literal">True</span>).shape <br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">(torch.Size([2, 3, 4]), torch.Size([3, 4]), torch.Size([1, 3, 4]))</code></pre></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 累计和</span><br>x.cumsum(axis=<span class="hljs-number">0</span>) ,x.cumsum(axis=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">(tensor([[[ 0.,  1.,  2.,  3.],          [ 4.,  5.,  6.,  7.],          [ 8.,  9., 10., 11.]],          [[12., 14., 16., 18.],          [20., 22., 24., 26.],          [28., 30., 32., 34.]]]), tensor([[[ 0.,  1.,  2.,  3.],          [ 4.,  6.,  8., 10.],          [12., 15., 18., 21.]],          [[12., 13., 14., 15.],          [28., 30., 32., 34.],          [48., 51., 54., 57.]]]))</code></pre></div><h1 id="7-第二种计算"><a href="#7-第二种计算" class="headerlink" title="7. 第二种计算"></a>7. 第二种计算</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 7.1 点积</span><br>x = torch.arange(<span class="hljs-number">5</span>,dtype=torch.float32)<br>y = x.clone()<br>x, y, torch.dot(x,y), torch.<span class="hljs-built_in">sum</span>(x*y)<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">(tensor([0., 1., 2., 3., 4.]), tensor([0., 1., 2., 3., 4.]), tensor(30.), tensor(30.))</code></pre></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 7.2 向量与矩阵</span><br>x = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>])<br>A = torch.arange(<span class="hljs-number">25</span>).reshape(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>)<br>torch.mv(A,x)<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">tensor([ 40, 115, 190, 265, 340])</code></pre></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 7.3 矩阵与矩阵</span><br>A = torch.arange(<span class="hljs-number">25</span>).reshape(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>)<br>B = torch.arange(<span class="hljs-number">15</span>).reshape(<span class="hljs-number">5</span>,<span class="hljs-number">3</span>)<br>torch.mm(A,B)<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">tensor([[ 90, 100, 110],        [240, 275, 310],        [390, 450, 510],        [540, 625, 710],        [690, 800, 910]])</code></pre></div><h1 id="8-范数"><a href="#8-范数" class="headerlink" title="8.范数"></a>8.范数</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 向量 L1范数</span><br>x = torch.tensor([<span class="hljs-number">1.0</span>,<span class="hljs-number">2.0</span>,-<span class="hljs-number">3.0</span>,<span class="hljs-number">4.0</span>])<br>torch.<span class="hljs-built_in">abs</span>(x).<span class="hljs-built_in">sum</span>()<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">tensor(10.)</code></pre></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 向量 L2范数</span><br>torch.norm(x)<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">tensor(5.4772)</code></pre></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 矩阵 F-范数</span><br>A = torch.arange(<span class="hljs-number">20</span>,dtype=torch.float32).reshape(<span class="hljs-number">5</span>,<span class="hljs-number">4</span>)<br>torch.norm(A)<br></code></pre></td></tr></table></figure><div class="code-wrapper"><pre><code class="hljs">tensor(49.6991)</code></pre></div>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
